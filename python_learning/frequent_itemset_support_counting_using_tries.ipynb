{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:**\n",
    "\n",
    "A prefix-tree is an ordered tree structure which can represent a transaction database in a highly compressed form. It is constructed by reading transactions one at a time with a predefined item order and mapping each transaction onto a path in the prefix-tree. Since different transactions can have several items in common, their paths may overlap.\n",
    "\n",
    "Assume a transaction DB of sequences. Write an efficient python implementation using a prefix tree to extract all sequence patterns that have a higher frequency than 'k'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's some amount of literature present on implementing prefix tree data structures efficiently for transaction dbs. My approach is: <br>\n",
    "- review what literature is available to get a high level overview\n",
    "- implement a prefix tree and finalise which tree architecture might make sense to you for grabbing itemset supports\n",
    "- implement a vanilla support counter - don't worry about optimising it here\n",
    "- iterate with different algorithms and strategies from different papers about performance gains from different data structure implementations\n",
    "\n",
    "Since I've done market basket analysis in the past, I understand the basic concepts and only need to understand the data structures. So timebox myself to 3 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature Review and understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www-users.cs.umn.edu/~kumar001/dmbook/ch6.pdf - excellent book for overall architecture\n",
    "- http://www.borgelt.net/papers/sam.pdf : split and merge algo by Chris Borgelt\n",
    "- https://linux.thai.net/~thep/datrie/datrie.html#WhatTake : two array implementation of a trie for better memory\n",
    "- https://ac.els-cdn.com/0895717703900586/1-s2.0-0895717703900586-main.pdf?_tid=24efd131-ea62-414d-bb03-3d4a2b6ffd90&acdnat=1526709387_78cce21acd95ae58bbdaca8cbf76538b : three array implementation to be used along with apriori\n",
    "- http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.87.2654&rep=rep1&type=pdf - excellent resource for trie implementation. use this\n",
    "- http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.87.2654&rep=rep1&type=pdf: same author trie based implementation. the guy uses far too many arrays though. how is this memory efficient?\n",
    "- https://wimleers.com/article/fp-growth-powered-association-rule-mining-with-support-for-constraints : fp growth. newer algo but faster. not much though. to be explored. i may not have time for this.\n",
    "- \"Efficient Implementations of Apriori and Eclat\" - solid paper when i skimmed through it, explaining the data structure at an implementation level. the guy also talks about another algo Eclat, which seems to have similar gains as fp growth.\n",
    "\n",
    "\n",
    "**Outcome: ** \n",
    "The vanilla implementation is obviously set up for failure because of combinatorial explosion when dealing with transaction db level of rows. We need some form of pruning for this data structure to be reasonably useful. So I will use the vanilla implementation as the baseline algorithm. \n",
    "\n",
    "Then, I'll start with implementing apriori algorithm for candidate generation and pruning. This is one of the oldest and most studied algorithms. I'm confident there's several production level tested implementations available. \n",
    "\n",
    "Ferenc Bodon's paper explains how Apriori  has reasonable traverse as compared to eclat memory requirements and outperforms fp growth when there are too many frequent items (which is probably the case for a medical knowledge base). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apriori algorithm implementation using APYORI\n",
    "\n",
    "To get a better sense of how the data structure might look, let me play around with a packaged implementation of apriori algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting apyori\n",
      "  Downloading https://files.pythonhosted.org/packages/25/fd/0561e2dd29aeed544bad2d1991636e38700cdaef9530490b863741f35295/apyori-1.1.1.tar.gz\n",
      "Building wheels for collected packages: apyori\n",
      "  Running setup.py bdist_wheel for apyori ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/vivekbharadwaj/Library/Caches/pip/wheels/7b/2a/35/c0c3749c1a36d4f454ea22d8396e1b854b86340d63cbbb7949\n",
      "Successfully built apyori\n",
      "Installing collected packages: apyori\n",
      "Successfully installed apyori-1.1.1\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install apyori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apyori import apriori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume the transaction database to be a list of lists. Each sub-list is a transaction row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_list = [['a','d'],\n",
    "        ['a','b','d'],\n",
    "        ['c','d'],\n",
    "        ['a','b','c','d'],\n",
    "        ['b','c'],\n",
    "        ['d'],\n",
    "        ['b','c','d']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate rows with minimum support, in this case we call it support of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[frozenset({'a'}),\n",
       " frozenset({'b'}),\n",
       " frozenset({'c'}),\n",
       " frozenset({'d'}),\n",
       " frozenset({'a', 'b'}),\n",
       " frozenset({'a', 'd'}),\n",
       " frozenset({'b', 'c'}),\n",
       " frozenset({'b', 'd'}),\n",
       " frozenset({'c', 'd'}),\n",
       " frozenset({'a', 'b', 'd'}),\n",
       " frozenset({'b', 'c', 'd'})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[0] for x in list(apriori(tx_list,min_support=2/len(tx_list)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fair enough. Use sets and possibly dict of lists. Some of the papers describe using pointers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports for coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from collections import defaultdict, OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointer based implementation of TRIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrieNode(object):\n",
    "    '''\n",
    "    Each node object is a parent with potential children and a support count \n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.transaction_complete = False\n",
    "        self.support_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TxTrie(object):\n",
    "    '''The entire transaction database which is a list of lists is represented \n",
    "        in a tree structure with a root node defined at depth 0, which points to \n",
    "        child nodes at depth d. Each node is represented by a label and pointers\n",
    "        to children\n",
    "       :param lst_Items_in_transaction: This is used in insert_items\n",
    "               represents each row of the transaction database which is ordered by frequency\n",
    "       :param candidate_items: This is used in search_items. In the apriori algo, a\n",
    "                pruned subset of candidate item sets generated from each row of the \n",
    "                transaction database is presented to the trie for traversal and search\n",
    "    '''    \n",
    "    \n",
    "    # initialising as root\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "        \n",
    "    def insert_items(self,lst_Items_in_transaction):\n",
    "        # initially refers to root instance. Later it will iterate over each item in the row\n",
    "        current_txn = self.root\n",
    "        \n",
    "        for item in lst_Items_in_transaction:\n",
    "            if item[1] not in current_txn.children:\n",
    "                current_txn.children[item[1]] = TrieNode()\n",
    "            # current transaction goes down the rabbit hole\n",
    "            current_txn = current_txn.children[item[1]]\n",
    "        current_txn.transaction_complete = True\n",
    "        \n",
    "    def search_items(self,candidate_items):\n",
    "        current_txn = self.root\n",
    "        \n",
    "        for item in candidate_items:\n",
    "            if item not in current_txn.children:\n",
    "                return 0\n",
    "            current_txn = current_txn.children[item]\n",
    "        if current_txn.transaction_complete == True:\n",
    "            current_txn.support_count += 1\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequencies(transactions, min_support=1):\n",
    "    '''this function is used to get a dict of relative frequencies across the entire database\n",
    "        used to generate the ordered list of trnasactions for our first traversal k=1\n",
    "    '''\n",
    "    frequencies = defaultdict(int)\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            frequencies[item] += 1\n",
    "    # we want to retain only frequent itemsets\n",
    "    return {k:v for k,v in frequencies.items() if v >= min_support}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sort_transactions_by_freq(\n",
    "        transactions, key_func=max, reverse_int=False,min_support=2,\n",
    "        reverse_ext=False, sort_ext=True):\n",
    "    '''ordering transactions by frequency, currently used for first traversal and number of \n",
    "        unique items but will be used for ordering every iteration in the future algos\n",
    "    '''\n",
    "    \n",
    "    key_seqs = [{key_func(i) for i in sequence} for sequence in transactions]\n",
    "    frequencies = get_frequencies(key_seqs)\n",
    "\n",
    "    asorted_seqs = []\n",
    "    for key_seq in key_seqs:\n",
    "        if not key_seq:\n",
    "            continue\n",
    "        # Sort each transaction (infrequent key first)\n",
    "        l = [(frequencies[i], i) for i in key_seq]\n",
    "        l.sort(reverse=reverse_int)\n",
    "        asorted_seqs.append(tuple(l))\n",
    "    # Sort all transactions. Those with infrequent key first, first\n",
    "    if sort_ext:\n",
    "        asorted_seqs.sort(reverse=reverse_ext)\n",
    "\n",
    "    return (asorted_seqs, frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_list = [['a', 'd'],\n",
    " ['a', 'b', 'd'],\n",
    " ['c', 'd'],\n",
    " ['a', 'b', 'c', 'd'],\n",
    " ['b', 'c'],\n",
    " ['d'],\n",
    " ['b', 'c', 'd']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_listSorted, blah = _sort_transactions_by_freq(tx_list,max)\n",
    "t = TxTrie()\n",
    "for txn in tx_listSorted:\n",
    "    t.insert_items(txn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((3, 'a'), (4, 'b'), (4, 'c'), (6, 'd')),\n",
       " ((3, 'a'), (4, 'b'), (6, 'd')),\n",
       " ((3, 'a'), (6, 'd')),\n",
       " ((4, 'b'), (4, 'c')),\n",
       " ((4, 'b'), (4, 'c'), (6, 'd')),\n",
       " ((4, 'c'), (6, 'd')),\n",
       " ((6, 'd'),)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx_listSorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.root.children['b'].children['c'].transaction_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.search_items(('b', 'c'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement baseline vanilla version from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate_itemset_generator(sorted_txn_list_row,num_of_items_itemset):\n",
    "    \n",
    "    lst_itemset = set()\n",
    "    \n",
    "    # same as before, if entire db is passed in the tx, then we assume a list of list\n",
    "    # otherwise we process a simple list\n",
    "    \n",
    "    temp = set(combinations([item[1] for item in sorted_txn_list_row], \n",
    "                            num_of_items_itemset))\n",
    "    for itemset_elements in temp:\n",
    "        lst_itemset.add(itemset_elements)\n",
    "    return lst_itemset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('a', 'b'), ('a', 'c'), ('a', 'd'), ('b', 'c'), ('b', 'd'), ('c', 'd')}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_itemset_generator(tx_listSorted[0],num_of_items_itemset=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d', 'a', 'b', 'c']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx_listSorted, F = _sort_transactions_by_freq(tx_list, min_support=2)\n",
    "list(F.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_apriori_vanilla(tx_list,min_support=2):\n",
    "    '''\n",
    "    this function creates a trie from the list of transactions.\n",
    "    then it traverses the transaction table multiple times (as a baseline to apriori)\n",
    "    and for each combination of itemsets (iterated by the number of items), I create \n",
    "    candidates in order to check support.\n",
    "    \n",
    "    I check support by running this against each transaction row and update support count\n",
    "    At the end, i prune candidates which are below my minimum support count\n",
    "    '''    \n",
    "    k=1; F=defaultdict(int);\n",
    "    tx_listSorted, F = _sort_transactions_by_freq(tx_list, min_support=min_support)\n",
    "    unique_items = list(F.keys())\n",
    "\n",
    "    # create prefix tree\n",
    "    t = TxTrie()\n",
    "    for txn in tx_listSorted:\n",
    "        t.insert_items(txn)\n",
    "        \n",
    "    while k<=len(unique_items):\n",
    "\n",
    "        for tx in tx_listSorted:    \n",
    "            tx_candidate_itemset = candidate_itemset_generator(tx, num_of_items_itemset=k)\n",
    "            for candidate in tx_candidate_itemset:\n",
    "                F.setdefault(candidate,0)\n",
    "                F[candidate] += t.search_items(candidate)\n",
    "                \n",
    "        # pruning candidates where support count is greater than min support value\n",
    "        F = {k:v for k,v in F.items() if v >= min_support}   \n",
    "        \n",
    "        k += 1\n",
    "        \n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('a', 'b', 'd'): 2,\n",
       " ('a', 'd'): 3,\n",
       " ('b', 'c'): 3,\n",
       " ('b', 'c', 'd'): 2,\n",
       " ('c', 'd'): 3,\n",
       " ('d',): 6,\n",
       " 'a': 3,\n",
       " 'b': 4,\n",
       " 'c': 4,\n",
       " 'd': 6}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blah = run_apriori_vanilla(tx_list,min_support=2)\n",
    "blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will obviously result in a combinatorial explosion. \n",
    "\n",
    "Next iteration will reduce the number of candidates being generated using different strategies. I will use the Fk−1×F1 Method where every frequent k-itemset is composed of a frequent (k−1)-itemset and a frequent 1-itemset. Hence the overall complexity of this step is:\n",
    "\n",
    "O(∑k|Fk−1||F1|)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_apriori(tx_list,min_support=2):\n",
    "    \n",
    "    # create all frequent 1-itemset and store them in an F-array\n",
    "    k=1; F=defaultdict(int);\n",
    "    tx_listSorted, F[k] = _sort_transactions_by_freq(tx_list, min_support=min_support)\n",
    "    \n",
    "    # create trie\n",
    "    t = TxTrie()\n",
    "    for txn in tx_listSorted:\n",
    "        t.insert_items(txn)\n",
    "        \n",
    "    while len(F[k])>0:\n",
    "\n",
    "        # candidate itemset store. \n",
    "        k+=1\n",
    "        \n",
    "        *** need to come up with a way to union them properly!!\n",
    "        F[k] = F[k-1] | F[k] \n",
    "        ***\n",
    "        \n",
    "        for tx in tx_listSorted:    \n",
    "            tx_candidate_itemset = candidate_itemset_generator(F[k], num_of_items_itemset=k)\n",
    "            for candidate in tx_candidate_itemset:\n",
    "                F[k][candidate] += t.search_items(candidate)\n",
    "                \n",
    "        # pruning candidates where support count is greater than min support value\n",
    "        F[k] = {k:v for k,v in support_count.items() if v >= min_support}   \n",
    "        \n",
    "        return F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of time. :(\n",
    "\n",
    "I've already spent more than 4 hours on this problem. Need to move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- too much superfluous traversing happens each time we scan the database. We can make sure of the ordering structure within a trie to order by frequencies and then store/ access based on it\n",
    "- use tabular representation using offset index as described in Chris Borgelt \"Efficient Implementation of apriori and eclat\"\n",
    "- employ early stopping if label is found\n",
    "- use intersection based pruning as described in Ference Bodon's \"Trie based Apriori implementation for mining frequent item sequences\"\n",
    "- At each node, store the length of the longest directed path that starts from it. When searching for k-itemset candidates, we can move downward only if the maximal path length at this node is k-d. This is explained in the \"Trie: an alternarive data structure for mining\" paper.\n",
    "- experiment with fp growth and SAM algorithms which you couldn't get to. SAM seems to have a relatively simpler implementation but you need to understand how to make the algorithm work with a trie offset array impleemntation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:google_api_py36]",
   "language": "python",
   "name": "conda-env-google_api_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
